behaviors:
  RobotArmTraining:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512
      buffer_size: 100
      learning_rate: 3.0e-4
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 3
      learning_rate_schedule: linear
      beta_schedule: constant
      epsilon_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 512
      num_layers: 16
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
      #curiosity:        #use for sparse rewards
      #  strength: 0.02
      #  gamma: 0.99
      #  encoding_size: 256
      #  learning_rate: 3.0e-4
    max_steps: 10000000
    time_horizon: 64
    summary_freq: 10000
environment_parameters:
  train_grabbing:
    curriculum:
      - name: Grabbing
        completion_criteria:
          measure: reward
          behavior: RobotArmTraining
          min_lesson_length: 0
          threshold: -1.0
        value: 0
      - name: PickingUp
        completion_criteria:
          measure: reward
          behavior: RobotArmTraining
          min_lesson_length: 0
          threshold: -1.0
        value: 1
      - name: Placing
        completion_criteria:
          measure: reward
          behavior: RobotArmTraining
          min_lesson_length: 100
          threshold: 1.0
        value: 2